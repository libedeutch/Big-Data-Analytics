{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## USE SPARK"
      ],
      "metadata": {
        "id": "-CQA14fYrtEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext"
      ],
      "metadata": {
        "id": "07xIlmJnea1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadMovieNames(data):\n",
        "  movieNames = {}\n",
        "  with open(data,encoding = 'latin-1') as f:\n",
        "    for line in f:\n",
        "      fields= line.split('|')\n",
        "      movieNames[int(fields[0])] = fields[1]\n",
        "  return movieNames\n",
        "def parseInput(line):\n",
        "  fields = line.split()\n",
        "  return (int(fields[1]), (float(fields[2]),1.0))"
      ],
      "metadata": {
        "id": "QJCIC9NIeg4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sc.parallelize([(\"coffee\", 1), (\"coffee\", 2), (\"tea\", 1), (\"coffee\", 3)])\n",
        "\n",
        "# Define a reduction function to sum the values\n",
        "def sum_values(x, y):\n",
        "  return x + y\n",
        "\n",
        "# Apply reduceByKey with the sum function\n",
        "reduced_data = data.reduceByKey(sum_values)\n",
        "\n",
        "# Print the result (might be shuffled due to partitioning)\n",
        "reduced_data.foreach(print)"
      ],
      "metadata": {
        "id": "OBxRt2dLjfWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averageRatings = ratingTotalsAndCount.mapValues(lambda totalAndCount: totalAndCount[0]/totalAndCount[1])\n",
        "\n",
        "\n",
        "# Sort by average values\n",
        "sortedMovies = averageRatings.sortBy(lambda x:x[1])\n",
        "\n",
        "# Take the top 10 results\n",
        "results = sortedMovies.take(10)\n",
        "\n",
        "# print them out\n",
        "for result in results:\n",
        "  print(movieNames[result[0]], result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGRHRky8KmdR",
        "outputId": "a81d53cc-8058-4c83-a305-4f0db396c37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amityville: Dollhouse (1996) 1.0\n",
            "Somebody to Love (1994) 1.0\n",
            "Every Other Weekend (1990) 1.0\n",
            "Homage (1995) 1.0\n",
            "3 Ninjas: High Noon At Mega Mountain (1998) 1.0\n",
            "Bird of Prey (1996) 1.0\n",
            "Power 98 (1995) 1.0\n",
            "Beyond Bedlam (1993) 1.0\n",
            "Falling in Love Again (1980) 1.0\n",
            "T-Men (1947) 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not want to use Python to submit spark code, use `spark-submit`  "
      ],
      "metadata": {
        "id": "lMgwi74mpF75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "oXzWmiTWdy9g",
        "outputId": "ec380594-63f8-4329-8dc8-6dd88eda477c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=WorstMovies, master=local[*]) created by __init__ at <ipython-input-11-b92979506560>:2 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-43691718f248>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# define spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WorstMovies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# laod up movie data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=WorstMovies, master=local[*]) created by __init__ at <ipython-input-11-b92979506560>:2 "
          ]
        }
      ],
      "source": [
        "if __name__==\"__main__\":\n",
        "\n",
        "  # define spark context\n",
        "  conf = SparkConf().setAppName(\"WorstMovies\")\n",
        "  sc = SparkContext(conf = conf)\n",
        "\n",
        "  # laod up movie data\n",
        "  movieNames  = loadMovieNames('u.item')\n",
        "\n",
        "  # load raw u.data file (could spread on whole cluster)\n",
        "  #lines = sc.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
        "  lines = sc.textFile(\"u.data\")\n",
        "\n",
        "\n",
        "  # Convert to (movieID, (rating,1.0))\n",
        "  movieRatings = lines.map(parseInput)\n",
        "\n",
        "\n",
        "  # reduce to (movieID, (sumOfRatings, totalRatings))\n",
        "  ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: (movie1[0] + movie2[0], movie1[1] + movie2[1]))\n",
        "\n",
        "  vc   = ratingTotalsAndCount.filter(lambda movie: movie[1][1] > 9 )\n",
        "  # Map to (movieID, averageRating)\n",
        "  averageRatings = ratingTotalsAndCount.mapValues(lambda totalAndCount: totalAndCount[0]/totalAndCount[1])\n",
        "\n",
        "\n",
        "  # Sort by average values\n",
        "  sortedMovies = averageRatings.sortBy(lambda x:x[1])\n",
        "\n",
        "  # Take the top 10 results\n",
        "  results = sortedMovies.take(10)\n",
        "\n",
        "  # print them out\n",
        "  for result in results:\n",
        "    print(movieNames[result[0]], result[1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use SPARK SQL and data frame"
      ],
      "metadata": {
        "id": "0bSnU6rCrk8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions"
      ],
      "metadata": {
        "id": "r-JyE2pzrkI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadMovieNames(data):\n",
        "  movieNames = {}\n",
        "  with open(data, encoding = 'latin-1') as f:\n",
        "    for line in f:\n",
        "      fields = line.split(\"|\")\n",
        "      movieNames[int(fields[0])] = fields[1]\n",
        "  return movieNames\n",
        "\n",
        "def parseInput(line):\n",
        "  fields = line.split()\n",
        "  # define a data frame\n",
        "  return Row(movieID = int(fields[1]), rating = float(fields[2]))"
      ],
      "metadata": {
        "id": "wnz7J6x_sLIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Create a SparkSession\n",
        "  spark = SparkSession.builder.appName(\"PopularMovie\").getOrCreate()\n",
        "\n",
        "  # Load up your movie ID -> name dictionary\n",
        "  movieNames = loadMovieNames('u.item')\n",
        "\n",
        "  # Get the raw data\n",
        "  lines = spark.sparkContext.textFile(\"u.data\")\n",
        "\n",
        "  # Convert it to (movieID, rating)\n",
        "  movies = lines.map(parseInput)\n",
        "\n",
        "\n",
        "  # Convert that to a DataFrame\n",
        "  movieDataset = spark.createDataFrame(movies)\n",
        "\n",
        "  # Compute average rating for each movieID\n",
        "  averageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n",
        "\n",
        "  # Compute count of rating for each movieID\n",
        "  counts = movieDataset.groupBy(\"movieID\").count()\n",
        "\n",
        "  # Join the two together (We now have movieID. avg(rating), and count column)\n",
        "  averageAndCounts = counts.join(averageRatings, \"movieID\")\n",
        "\n",
        "\n",
        "  # print(averageAndCount.columns)\n",
        "  data_with_names = averageAndCounts.withColumnRenamed(\"movieID\", \"col1\") \\\n",
        "                     .withColumnRenamed(\"count\", \"col2\") \\\n",
        "                     .withColumnRenamed(\"avg(rating)\", \"col3\")\n",
        "\n",
        "  data_with_names = data_with_names.filter(data_with_names.col2>9 )\n",
        "  # PULL the top 10 results\n",
        "  #topten = averageAndCounts.orderBy(\"avg(rating)\").take(10)\n",
        "  topten = data_with_names.orderBy( data_with_names.col3.asc(), data_with_names.col2.asc()).take(10)\n",
        "\n",
        "  # Print them out, converting movieID to names\n",
        "\n",
        "  for movie in topten:\n",
        "    print(movieNames[movie[0]], movie[1], movie[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBepQRq_s6ww",
        "outputId": "d2198b76-6f2b-498d-faf2-7fceceaa2cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Children of the Corn: The Gathering (1996) 19 1.3157894736842106\n",
            "Body Parts (1991) 13 1.6153846153846154\n",
            "Amityville II: The Possession (1982) 14 1.6428571428571428\n",
            "Bloodsport 2 (1995) 10 1.7\n",
            "Lawnmower Man 2: Beyond Cyberspace (1996) 21 1.7142857142857142\n",
            "Robocop 3 (1993) 11 1.7272727272727273\n",
            "Free Willy 3: The Rescue (1997) 27 1.7407407407407407\n",
            "Kazaam (1996) 10 1.8\n",
            "Gone Fishin' (1997) 11 1.8181818181818181\n",
            "Solo (1996) 12 1.8333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "averageAndCounts = counts.join(averageRatings, \"movieID\")\n",
        "print(averageAndCounts.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4_Cjk0SM6zI",
        "outputId": "3f795b04-9ab2-4d3d-cb51-11f7cc884a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movieID', 'count', 'avg(rating)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ne3rmZfuM_C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "y7MDLy0qM_Ji",
        "outputId": "2a3e9896-bc6c-429a-a1e0-f016fa95ec34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got slice.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ba97729cde9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverageAndCounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \"\"\"\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2763\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m     def _sort_cols(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2749\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2750\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NOT_COLUMN_OR_STR\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got slice."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use `MLlib` build a recommendation system"
      ],
      "metadata": {
        "id": "QDEcjRs64P15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lit\n"
      ],
      "metadata": {
        "id": "hquwMpyzzzoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load movies\n",
        "def loadMovieNames(data):\n",
        "  movieNames = {}\n",
        "  with open(data, encoding = 'latin-1') as f:\n",
        "    for line in f:\n",
        "      fields = line.split(\"|\")\n",
        "      movieNames[int(fields[0])] = fields[1] # fields[1].decode('ascii', 'ignore')\n",
        "  return movieNames\n",
        "\n",
        "def parseInput(line):\n",
        "  fields = line.value.split()\n",
        "  return Row(userID = int(fields[0]), movieID = int(fields[1]), rating = float(fields[2]) )\n"
      ],
      "metadata": {
        "id": "22_8HFtS4yQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "if __name__ == \"__main__\":\n",
        "  # Create  a SparkSession\n",
        "  spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
        "\n",
        "  # Load up out movie ID\n",
        "  movieNames = loadMovieNames('u.item')\n",
        "\n",
        "  # Get the raw data\n",
        "  lines = spark.read.text(\"u.data\").rdd # why\n",
        "\n",
        "  # convert it to a RDD of Row object with (userID, movieID, rating)\n",
        "  ratingsRDD = lines.map(parseInput)\n",
        "\n",
        "  # Convert to a DataFrame and cache it\n",
        "  ratings = spark.createDataFrame(ratingsRDD).cache() # caling cache because we will use it more than once\n",
        "\n",
        "  # Create an ALS collaborative filtering model from the complete data set\n",
        "  als = ALS(maxIter = 5, regParam = 0.01, userCol = \"userID\", itemCol = \"movieID\", ratingCol = \"rating\")\n",
        "  model = als.fit(ratings)\n",
        "\n",
        "\n",
        "  # print out ratings from user 0\n",
        "  userRatings = ratings.filter(\"userID = 0\")\n",
        "  for rating in userRatings.collect():\n",
        "    print(movieNames[rating['movieID']], rating['rating'])\n",
        "\n",
        "  print(\"\\nTop 20 recommendations:\")\n",
        "\n",
        "  # Find movies rated more than 100 times\n",
        "  ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count>100\")\n",
        "  # Construct a \"test\" dataframe for user 0 with every movie rated more than 100 times\n",
        "  popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(0))\n",
        "  # second column is userID = 0, means we want to browse through every movie rate more than\n",
        "  # 100 times, and predict the ratings of user 0\n",
        "\n",
        "\n",
        "  recommendations = model.transform(popularMovies)\n",
        "  top20 = recommendations.orderBy(recommendations.prediction.desc(), recommendations.movieID.asc()).take(20)\n",
        "  # orderBy or sort\n",
        "\n",
        "  for re in top20:\n",
        "    print(movieNames[re[0]], re[2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpqZpkz4zn14",
        "outputId": "dc34c37d-d240-4069-dd03-a4b271b58f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Star Wars (1977) 5.0\n",
            "Empire Strikes Back, The (1980) 5.0\n",
            "Gone with the Wind (1939) 1.0\n",
            "\n",
            "Top 20 recommendations:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  top20 = recommendations.orderBy(recommendations.prediction.desc(), recommendations.movieID.asc()).take(20)\n",
        "\n",
        "\n",
        "  for re in top20:\n",
        "    print(movieNames[re[0]], re[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mop-u43--DA",
        "outputId": "c15f5ced-ed1a-431c-9b65-c55966a0eb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ace Ventura: Pet Detective (1994) 6.27745246887207\n",
            "Nightmare on Elm Street, A (1984) 5.941840648651123\n",
            "Princess Bride, The (1987) 5.611507415771484\n",
            "Army of Darkness (1993) 5.588329315185547\n",
            "Beavis and Butt-head Do America (1996) 5.561034679412842\n",
            "Austin Powers: International Man of Mystery (1997) 5.423329830169678\n",
            "Clerks (1994) 5.343860149383545\n",
            "Star Trek: The Wrath of Khan (1982) 5.244342803955078\n",
            "Blues Brothers, The (1980) 5.182976722717285\n",
            "Terminator, The (1984) 5.093161106109619\n",
            "Die Hard (1988) 5.047311305999756\n",
            "Seven (Se7en) (1995) 5.028848648071289\n",
            "Star Wars (1977) 4.998826026916504\n",
            "Happy Gilmore (1996) 4.973372936248779\n",
            "Highlander (1986) 4.960333824157715\n",
            "Batman (1989) 4.954259395599365\n",
            "Empire Strikes Back, The (1980) 4.953100681304932\n",
            "Mystery Science Theater 3000: The Movie (1996) 4.9426374435424805\n",
            "Monty Python and the Holy Grail (1974) 4.932487487792969\n",
            "Aliens (1986) 4.852281093597412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userRatings = ratings.filter(\"userID = 0\")\n",
        "for rating in userRatings.collect():\n",
        "    print(movieNames[rating['movieID']], rating['rating'])"
      ],
      "metadata": {
        "id": "k_MNaga_zZuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgDQFmr1x_sY",
        "outputId": "814a07be-cf86-4bd1-c89a-d5094d4b15a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(movieID=474, count=194, avg(rating)=4.252577319587629),\n",
              " Row(movieID=29, count=114, avg(rating)=2.6666666666666665),\n",
              " Row(movieID=26, count=73, avg(rating)=3.452054794520548),\n",
              " Row(movieID=964, count=9, avg(rating)=3.3333333333333335),\n",
              " Row(movieID=65, count=115, avg(rating)=3.5391304347826087),\n",
              " Row(movieID=191, count=276, avg(rating)=4.163043478260869),\n",
              " Row(movieID=1224, count=12, avg(rating)=2.6666666666666665),\n",
              " Row(movieID=558, count=70, avg(rating)=3.6714285714285713),\n",
              " Row(movieID=1010, count=44, avg(rating)=3.25),\n",
              " Row(movieID=418, count=129, avg(rating)=3.5813953488372094)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qJRldDZHrr2Y"
      }
    }
  ]
}